# The Innovators

![](https://is4-ssl.mzstatic.com/image/thumb/Publication18/v4/8e/f3/bb/8ef3bb15-e840-f23e-da41-08fb6772f8fe/9781476708713.jpg/1400x2101bb.jpeg)

### Metadata

- Author: Walter Isaacson
- Full Title: The Innovators
- Category: #books

### Highlights

- Instead, most of the innovations of the digital age were done collaboratively. (Location 195)
- It’s also a narrative of how they collaborated and why their ability to work as teams made them even more creative. (Location 198)
- I also became interested in how the quest for artificial intelligence—machines that think on their own—has consistently proved less fruitful than creating ways to forge a partnership or symbiosis between people and machines. (Location 254)
- I was struck by how the truest creativity of the digital age came from those who were able to connect the arts and sciences. (Location 257)
- Leonardo da Vinci was the exemplar of the creativity that flourishes when the humanities and sciences interact. (Location 263)
- The first was that of a general-purpose machine, one that could not only perform a preset task but could be programmed and reprogrammed to do a limitless and changeable array of tasks. In other words, she envisioned the modern computer. (Location 594)
- Ada’s second noteworthy concept sprang from this description of a general-purpose machine. Its operations, she realized, did not need to be limited to math and numbers. (Location 611)
- she noted that a machine such as the Analytical Engine could store, manipulate, process, and act upon anything that could be expressed in symbols: words and logic and music and anything else we might use symbols to convey. (Location 613)
- other than mathematical quantities. Thus did she make the conceptual leap from machines that were mere calculators to ones that we now call computers. (Location 625)
- The Industrial Revolution was based on two grand concepts that were profound in their simplicity. Innovators came up with ways to simplify endeavors by breaking them into easy, small tasks that could be accomplished on assembly lines. Then, beginning in the textile industry, inventors found ways to mechanize steps so that they could be performed by machines, many of them powered by steam engines. Babbage, building on ideas from Pascal and Leibniz, tried to apply these two processes to the production of computations, creating a mechanical precursor to the modern computer. (Location 719)
- “In considering any new subject, there is frequently a tendency, first, to overrate what we find to be already interesting or remarkable; and, secondly, by a sort of natural reaction, to undervalue the true state of the case.” (Location 730)
- Sometimes innovation is a matter of timing. A big idea comes along at just the moment when the technology exists to implement it. (Location 751)
- Charles Babbage published his paper about a sophisticated computer in 1837, but it took a hundred years to achieve the scores of technological advances needed to build one. (Location 754)
- Perfecting the use of punch cards for computers came about because Herman Hollerith, an employee of the U.S. Census Bureau, was appalled that it took close to eight years to manually tabulate the 1880 census. He resolved to automate the 1890 count. (Location 757)
- It was the first major use of electrical circuits to process information, and the company that Hollerith founded became in 1924, after a series of mergers and acquisitions, the International Business Machines Corporation, or IBM. (Location 764)
- Another approach to computing was to build devices that could mimic or model a physical phenomenon and then make measurements on the analogous model to calculate the relevant results. These were known as analog computers because they worked by analogy. Analog computers do not rely on discrete integers to make their calculations; instead, they use continuous functions. In analog computers, a variable quantity such as electrical voltage, the position of a rope on a pulley, hydraulic pressure, or a measurement of distance is employed as an analog for the corresponding quantities of the problem to be solved. A slide rule is analog; an abacus is digital. (Location 776)
- That challenge of linking together multiple integrators was not mastered until 1931, when an MIT engineering professor, Vannevar (rhymes with beaver) Bush—remember his name, for he is a key character in this book—was able to build the world’s first analog electrical-mechanical computer. (Location 790)
- It would not be until the 2010s that computer scientists, seeking to mimic the human brain, would seriously begin working on ways to revive analog computing. (Location 805)
- Like many mathematical concepts, binary theory was pioneered by Leibniz in the late seventeenth century. (Location 808)
- In the mid-1930s, the British engineer Tommy Flowers pioneered the use of vacuum tubes as on-off switches in electronic circuits. (Location 811)
- Vacuum tubes had mainly been employed to amplify signals rather than as on-off switches. (Location 813)
- The development of vacuum tubes for the radio industry paved the way for the creation of electronic digital circuits. That was accompanied by theoretical advances in logic that made circuits more useful. And the march was quickened by the drums of war. As nations began arming for the looming conflict, it became clear that computational power was as important as firepower. (Location 821)
- It is somewhat like the ancient Greek “liar’s paradox,” in which the truth of the statement “This statement is false” cannot be determined. (If the statement is true, then it’s also false, and vice versa.) (Location 890)
- “It is possible to invent a single machine which can be used to compute any computable sequence,” he declared.10 Such a machine would be able to read the instructions of any other machine and carry out whatever task that machine could do. In essence, it embodied the dream of Charles Babbage and Ada Lovelace for a completely general-purpose universal machine. (Location 926)
- At Bell Labs, Shannon saw up close the wonderful power of the phone system’s circuits, which used electrical switches to route calls and balance loads. In his mind, he began connecting the workings of these circuits to another subject he found fascinating, the system of logic formulated ninety years earlier by the British mathematician George Boole. (Location 965)
- Shannon figured out that electrical circuits could execute these logical operations using an arrangement of on-off switches. To perform an and function, for example, two switches could be put in sequence, so that both had to be on for electricity to flow. To perform an or function, the switches could be in parallel so that electricity would flow if either of them was on. Slightly more versatile switches called logic gates could streamline the process. In other words, you could design a circuit containing a lot of relays and logic gates that could perform, step by step, a sequence of logical tasks. (Location 970)
- Stibitz’s computer was not programmable, but it showed the potential of a circuit of relays to do binary math, process information, and handle logical procedures. (Location 1000)
- “The desire to economize time and mental effort in arithmetical computations, and to eliminate human liability to error is probably as old as the science of arithmetic itself,” his memo began. (Location 1011)
- It did, however, have one impressive feature that would become a staple of modern computers: it was fully automatic. Programs and data were entered by paper tape, and it could run for days with no human intervention. That allowed Aiken to refer to it as “Babbage’s dream come true.” (Location 1040)
- So Zuse, like so many others, was driven by the desire to mechanize the tedious process of solving mathematical equations. (Location 1050)
- How did they develop this idea at the same time when war kept their two teams isolated? The answer is partly that advances in technology and theory made the moment ripe. Along with many other innovators, Zuse and Stibitz were familiar with the use of relays in phone circuits, and it made sense to tie that to binary operations of math and logic. Likewise, Shannon, who was also very familiar with phone circuits, made the related theoretical leap that electronic circuits would be able to perform the logical tasks of Boolean algebra. The idea that digital circuits would be the key to computing was quickly becoming clear to researchers almost everywhere, even in isolated places like central Iowa. (Location 1075)
- Nevertheless, John Vincent Atanasoff, known to his wife and friends as Vincent, deserves the distinction of being the pioneer who conceived the first partly electronic digital computer, and he did so after he was struck by inspiration during a long impetuous drive one night in December 1937. (Location 1084)
- The first problem he tackled was how to store numbers in a machine. He used the term memory to describe this feature: “At the time, I had only a cursory knowledge of the work of Babbage and so did not know he called the same concept ‘store.’ . . . I like his word, and perhaps if I had known, I would have adopted it; I like ‘memory,’ too, with its analogy to the brain.” (Location 1113)
- The fastest would be vacuum tubes, but they were expensive. So he opted instead to use what he called condensers—what we now call capacitors—which are small and inexpensive components that can store, at least briefly, an electrical charge. (Location 1118)
- He decided it should be fully electronic; that meant using vacuum tubes, even though they were expensive. The tubes would act as on-off switches to perform the function of logic gates in a circuit that could add, subtract, and perform any Boolean function. (Location 1130)
- At that point, work stopped. Atanasoff was drafted into the Navy and sent to its ordnance laboratory in Washington, DC, where he worked on acoustic mines and later attended the atomic bomb tests at Bikini Atoll. Shifting his focus from computers to ordnance engineering, he remained an inventor, earning thirty patents, including on a minesweeping device. But his Chicago lawyer never applied for patents on his computer. (Location 1176)
- Atanasoff’s enduring romantic appeal is that he was a lone tinkerer in a basement, with only his young sidekick Clifford Berry for a companion. But his tale is evidence that we shouldn’t in fact romanticize such loners. Like Babbage, who also toiled in his own little workshop with just an assistant, Atanasoff never got his machine to be fully functional. Had he been at Bell Labs, amid swarms of technicians and engineers and repairmen, or at a big research university, a solution would likely have been found for fixing the card reader as well as the other balky parts of his contraption. Plus, when Atanasoff was called away to the Navy in 1942, there would have been team members left behind to put on the finishing touches, or at least to remember what was being built. (Location 1187)
- Mauchly did what good innovators properly do: he drew upon all of the information he had picked up from his travels. (Location 1247)
- pulling off a sheet to reveal what he and Berry were cobbling (Location 1282)
- “A new idea comes suddenly and in a rather intuitive way,” Einstein once said, “but intuition is nothing but the outcome of earlier intellectual experience.” (Location 1304)
- Unlike Atanasoff, Mauchly had the opportunity, and the inclination, to collaborate with a team filled with varied talents. As a result, instead of producing a machine that didn’t quite work and was abandoned in a basement, he and his team would go down in history as the inventors of the first electronic general-purpose computer. (Location 1310)
- He had been accepted into an electronics course at the University of Pennsylvania, one of the many around the country being funded on an emergency basis by the War Department. It was a chance to learn more about using vacuum tubes in electronic circuits, which he was now convinced was the best way to make computers. It also showed the importance of the military in driving innovation in the digital age. (Location 1313)
- “A physicist is one who’s concerned with the truth,” he later said. “An engineer is one who’s concerned with getting the job done.” (Location 1368)
- War mobilizes science. Over the centuries, ever since the ancient Greeks built a catapult and Leonardo da Vinci served as the military engineer for Cesare Borgia, martial needs have propelled advances in technology, and this was especially true in the mid-twentieth century. Many of the paramount technological feats of that era—computers, atomic power, radar, and the Internet—were spawned by the military. (Location 1370)
- The decision of the U.S. War Department to fund the electronic computer came on April 9, 1943. (Location 1392)
- Soon it was given a more memorable name: ENIAC, the Electronic Numerical Integrator and Computer. Even though ENIAC was designed primarily for handling differential equations, which were key to calculating missile trajectories, Mauchly wrote that it could have a “programming device” that would allow it to do other tasks, thus making it more of a general-purpose computer. (Location 1403)
- Eckert and Mauchly served as counterbalances for each other, which made them typical of so many digital-age leadership duos. (Location 1416)
- The computer, known as Colossus, was the first all-electronic, partially programmable computer. (Location 1438)
- Its first decoded intercepts supported other sources informing General Dwight Eisenhower, who was about to launch the D-Day invasion, that Hitler was not ordering extra troops to Normandy. Within a year, eight more Colossus machines were produced. (Location 1493)
- This meant that well before ENIAC, which did not become operational until November 1945, the British code breakers had built a fully electronic and digital (indeed binary) computer. The second version, in June 1944, was even capable of some conditional branching. But unlike ENIAC, which had ten times the number of tubes, Colossus was a special-purpose machine geared for code breaking, not a general-purpose computer. With its limited programmability, it could not be instructed to perform all computational tasks, the way that (in theory) ENIAC could. (Location 1495)
- ENIAC, completed by Presper Eckert and John Mauchly in November 1945, was the first machine to incorporate the full set of traits of a modern computer. It was all-electronic, superfast, and could be programmed by plugging and unplugging the cables connecting its different units. It was capable of changing paths based on interim results, and it qualified as a general-purpose Turing-complete machine, meaning it could in theory tackle any task. Most important, it worked. (Location 1526)
- That last attribute is important. When we ascribe credit for an invention, determining who should be most noted by history, one criterion is looking at whose contributions turned out to have the most influence. Invention implies contributing something to the flow of history and affecting how an innovation developed. Using historic impact as a standard, Eckert and Mauchly are the most noteworthy innovators. (Location 1532)
- That raised another criterion, more legalistic than historical, in assessing credit for invention: Who, if anyone, ended up with the patents? In the case of the first computers, nobody did. (Location 1540)
- Pursuing a tip from a Honeywell lawyer who had gone to Iowa State and read about the computer that Atanasoff had built there, Call paid a visit to Atanasoff at his home in Maryland. Atanasoff was charmed by Call’s knowledge of his computer and somewhat resentful that he had never gotten much credit for it, so he handed over hundreds of letters and documents that showed how Mauchly had derived some ideas from his visit to Iowa. That evening Call drove to Washington to sit in the back of a lecture Mauchly was giving. In answer to a question about Atanasoff’s machine, Mauchly claimed he had barely examined it. Call realized that if he could get Mauchly to say this in a deposition, then he could discredit him at a trial by producing Atanasoff’s documents. (Location 1548)
- In it he ruled that the Eckert-Mauchly ENIAC patent was invalid: “Eckert and Mauchly did not themselves first invent the automatic electronic digital computer, but instead derived that subject matter from one Dr. John Vincent Atanasoff.” (Location 1563)
- it resurrected Atanasoff from the basement of history, and it showed very clearly, though this was not the intent of the judge or either party, that great innovations are usually the result of ideas that flow from a large number of sources. An invention, especially one as complex as the computer, usually comes not from an individual brainstorm but from a collaboratively woven tapestry of creativity. (Location 1571)
- Mauchly and Eckert should be at the top of the list of people who deserve credit for inventing the computer, not because the ideas were all their own but because they had the ability to draw ideas from multiple sources, add their own innovations, execute their vision by building a competent team, and have the most influence on the course of subsequent developments. (Location 1575)
- How you rank the historic contributions of the others depends partly on the criteria you value. If you are enticed by the romance of lone inventors and care less about who most influenced the progress of the field, you might put Atanasoff and Zuse high. But the main lesson to draw from the birth of computers is that innovation is usually a group effort, involving collaboration between visionaries and engineers, and that creativity comes from drawing on many sources. Only in storybooks do inventions come like a thunderbolt, or a lightbulb popping out of the head of a lone individual in a basement or garret or garage. (Location 1581)
- All of the machines built during the war were conceived, at least initially, with a specific task in mind, such as solving equations or deciphering codes. A real computer, like that envisioned by Ada Lovelace and then Alan Turing, should be able to perform, seamlessly and quickly, any logical operation. (Location 1604)
- “We do not need to have an infinity of different machines doing different jobs,” he wrote in 1948. “A single one will suffice. The engineering problem of producing various machines for various jobs is replaced by the office work of ‘programming’ the universal machine to do these jobs.” (Location 1607)
- In her probability course, she began with a lecture on one of her favorite mathematical formulasI and asked her students to write an essay about it. These she would mark for clarity of writing and style. “I’d cover [an essay] up with ink, and I would get a rebellion that they were taking a math course not an English course,” she recalled. “Then I would explain, it was no use trying to learn math unless they could communicate it with other people.”4 Throughout her life, she excelled at being able to translate scientific problems—such as those involving trajectories, fluid flows, explosions, and weather patterns—into mathematical equations and then into ordinary English. This talent helped to make her a good programmer. (Location 1629)
- Because of her ability to communicate precisely, Aiken assigned her to write what was to become the world’s first computer programming manual. “You are going to write a book,” he said one day, standing next to her desk. (Location 1654)
- “He pointed out that if you stumble when you try to read it aloud, you’d better fix that sentence. Every day I had to read five pages of what I had written.” (Location 1665)
- The more she learned about Ada Lovelace, the more Hopper identified with her. “She wrote the first loop,” Hopper said. “I will never forget. None of us ever will.” (Location 1668)
- “The locus of technological innovation, according to IBM, was the corporation. The myth of the lone radical inventor working in the laboratory or basement was replaced by the reality of teams of faceless organizational engineers contributing incremental advancements.” (Location 1673)
- The difference between Hopper’s version of history and IBM’s ran deeper than a dispute over who should get the most credit. It showed fundamentally contrasting outlooks on the history of innovation. Some studies of technology and science emphasize, as Hopper did, the role of creative inventors who make innovative leaps. Other studies emphasize the role of teams and institutions, such as the collaborative work done at Bell Labs and IBM’s Endicott facility. This latter approach tries to show that what may seem like creative leaps—the Eureka moment—are actually the result of an evolutionary process that occurs when ideas, concepts, technologies, and engineering methods ripen together. Neither way of looking at technological advancement is, on its own, completely satisfying. Most of the great innovations of the digital age sprang from an interplay of creative individuals (Mauchly, Turing, von Neumann, Aiken) with teams that knew how to implement their ideas. (Location 1678)
- Hopper’s approach to programming was very systematic. She broke down every physics problem or mathematical equation into small arithmetic steps. “You simply step by step told the computer what to do,” she explained. “Get this number and add it to that number and put the answer there. Now pick up this number and multiply it by this number and put it there.”12 When the program was punched into a tape and the moment came to test it, the Mark I crew, as a joke that became a ritual, would pull out a prayer rug, face east, and pray that their work would prove acceptable. (Location 1691)
- In addition, her crew helped to popularize the terms bug and debugging. The Mark II version of the Harvard computer was in a building without window screens. One night the machine conked out, and the crew began looking for the problem. They found a moth with a wingspan of four inches that had gotten smashed in one of the electromechanical relays. It was retrieved and pasted into the log book with Scotch tape. “Panel F (moth) in relay,” the entry noted. “First actual case of bug being found.”18 From then on, they referred to ferreting out glitches as “debugging the machine.” (Location 1718)
- Jean was the sixth of seven children, all of whom went to college. That was back when state governments valued education and realized the economic and social value of making it affordable. She attended Northwest Missouri State Teachers College in Maryville, where the tuition was $76 per year. (In 2013 it was approximately $14,000 per year for in-state residents, a twelve-fold increase after adjusting for inflation.) (Location 1756)
- Shortly before she died in 2011, Jean Jennings Bartik reflected proudly on the fact that all the programmers who created the first general-purpose computer were women: “Despite our coming of age in an era when women’s career opportunities were generally quite confined, we helped initiate the era of the computer.” It happened because a lot of women back then had studied math, and their skills were in demand. There was also an irony involved: the boys with their toys thought that assembling the hardware was the most important task, and thus a man’s job. “American science and engineering was even more sexist than it is today,” Jennings said. “If the ENIAC’s administrators had known how crucial programming would be to the functioning of the electronic computer and how complex it would prove to be, they might have been more hesitant to give such an important role to women.” (Location 1827)
- Von Neumann was another innovator who stood at the intersection of the humanities and sciences. (Location 1866)
- Von Neumann impressed the Harvard team with how collaborative he was. He absorbed their ideas, took credit for some, but also made it clear that nobody should claim ownership of any concept. (Location 1913)
- “The stored-program computer, as conceived by Alan Turing and delivered by John von Neumann, broke the distinction between numbers that mean things and numbers that do things,” George Dyson wrote. (Location 1969)
- The sparks come from ideas rubbing against each other rather than as bolts out of the blue. (Location 1995)
- In 2011 a milestone was reached: Apple and Google spent more on lawsuits and payments involving patents than they did on research and development of new products. (Location 2035)
- “Perhaps institutions as well as people can become fatigued,” (Location 2143)
- Instead, beginning in the 1950s, innovation in computing shifted to the corporate realm, (Location 2187)
- “No, I’m not interested in developing a powerful brain. All I’m after is just a mediocre brain, something like the President of the American Telephone and Telegraph Company.” (Location 2220)
- Turing’s test, which he called “the imitation game,” is simple: An interrogator sends written questions to a human and a machine in another room and tries to determine from their answers which one is the human. (Location 2250)
- He spoke so much about how sexual appetites affected human thinking that the BBC editors cut some of it out of the broadcast, including his assertion that he would not believe a machine could think until he saw it touch the leg of a female machine. (Location 2320)
- At the trial in March 1952, Turing pled guilty, though he made clear he felt no remorse. Max Newman appeared as a character witness. Convicted and stripped of his security clearance,VI Turing was offered a choice: imprisonment or probation contingent on receiving hormone treatments via injections of a synthetic estrogen designed to curb his sexual desires, as if he were a chemically controlled machine. He chose the latter, which he endured for a year. Turing at first seemed to take it all in stride, but on June 7, 1954, he committed suicide by biting into an apple he had laced with cyanide. His friends noted that he had always been fascinated by the scene in Snow White in which the Wicked Queen dips an apple into a poisonous brew. He was found in his bed with froth around his mouth, cyanide in his system, and a half-eaten apple by his side. (Location 2334)
- The transistor, as the device was soon named, became to the digital age what the steam engine was to the Industrial Revolution. (Location 2377)
- “It had become a matter of some consideration at the Labs whether the key to invention was a matter of individual genius or collaboration,” Jon Gertner wrote in The Idea Factory, a study of Bell Labs.2 The answer was both. “It takes many men in many fields of science, pooling their various talents, to funnel all the necessary research into the development of one new device,” Shockley later explained.3 He was right. He was also, however, showing a rare flash of feigned humility. More than anyone, he believed in the importance of the individual genius, such as himself. Even Kelly, the proselytizer for collaboration, realized that individual genius also needed to be nurtured. “With all the needed emphasis on leadership, organization and teamwork, the individual has remained supreme—of paramount importance,” he once said. “It is in the mind of a single person that creative ideas and concepts are born.” (Location 2411)
- The key to innovation—at Bell Labs and in the digital age in general—was realizing that there was no conflict between nurturing individual geniuses and promoting collaborative teamwork. It was not either-or. Indeed, throughout the digital age, the two approaches went together. Creative geniuses (John Mauchly, William Shockley, Steve Jobs) generated innovative ideas. Practical engineers (Presper Eckert, Walter Brattain, Steve Wozniak) partnered closely with them to turn concepts into contraptions. And collaborative teams of technicians and entrepreneurs worked to turn the invention into a practical product. When part of this ecosystem was lacking, such as for John Atanasoff at Iowa State or Charles Babbage in the shed behind his London home, great concepts ended up being consigned to history’s basement. And when great teams lacked passionate visionaries, such as Penn after Mauchly and Eckert left, Princeton after von Neumann, or Bell Labs after Shockley, innovation slowly withered. (Location 2419)
- It met once a week in the late afternoon to share findings, engage in a bit of academic-style trash talk, and then adjourn for informal discussions that lasted late into the night. There was value to getting together in person rather than just reading each other’s papers: the intense interactions allowed ideas to be kicked into higher orbits and, like electrons, occasionally break loose to spark chain reactions. (Location 2446)
- the triangular relationship that was forged among the government, research universities, and private industry. (Location 2494)
- Innovation happens in stages. In the case of the transistor, first there was the invention, led by Shockley, Bardeen, and Brattain. Next came the production, led by engineers such as Teal. Finally, and equally important, there were the entrepreneurs who figured out how to conjure up new markets. Teal’s plucky boss Pat Haggerty was a colorful case study of this third step in the innovation process. (Location 2711)
- The Regency radio, the size of a pack of index cards, used four transistors and sold for $49.95. It was initially marketed partly as a security item, now that the Russians had the atom bomb. “In event of an enemy attack, your Regency TR-1 will become one of your most valued possessions,” the first owner’s manual declared. But it quickly became an object of consumer desire and teenage obsession. Its plastic case came, iPod-like, in four colors: black, ivory, Mandarin Red, and Cloud Gray. Within a year, 100,000 had been sold, making it one of the most popular new products in history. (Location 2722)
- More fundamentally, the transistor radio became the first major example of a defining theme of the digital age: technology making devices personal. The radio was no longer a living-room appliance to be shared; it was a personal device that allowed you to listen to your own music where and when you wanted—even if it was music that your parents wanted to ban. (Location 2729)
- Indeed, there was a symbiotic relationship between the advent of the transistor radio and the rise of rock and roll. Elvis Presley’s first commercial recording, “That’s All Right,” came out at the same time as the Regency radio. The rebellious new music made every kid want a radio. And the fact that the radios could be taken to the beach or the basement, away from the disapproving ears and dial-controlling fingers of parents, allowed the music to flourish. (Location 2732)
- The seeds were planted for a shift in perception of electronic technology, especially among the young. It would no longer be the province only of big corporations and the military. It could also empower individuality, personal freedom, creativity, and even a bit of a rebellious spirit. (Location 2738)
- Another skill of great team leaders is the ability to instill a nonhierarchical esprit de corps. (Location 2744)
- The civilian space program, along with the military program to build ballistic missiles, propelled the demand for both computers and transistors. It also helped assure that the development of these two technologies became linked. (Location 3025)
- So who invented the microchip? As with the question of who invented the computer, the answer cannot be settled simply by reference to legal rulings. The nearly simultaneous advances made by Kilby and Noyce showed that the atmosphere of the time was primed for such an invention. (Location 3198)
- The first major market for microchips was the military. In 1962 the Strategic Air Command designed a new land-based missile, the Minuteman II, that would each require two thousand microchips just for its onboard guidance system. Texas Instruments won the right to be the primary supplier. (Location 3219)
- America’s civilian space program was the next big booster for microchip production. In May 1961 President John F. Kennedy declared, “I believe that this nation should commit itself to achieving the goal, before this decade is out, of landing a man on the moon and returning him safely to the earth.” The Apollo program, as it became known, needed a guidance computer that could fit into a nose cone. (Location 3229)
- Eleven years after he had created a huge market for inexpensive transistors by pushing pocket radios, he looked for a way to do the same for microchips. The idea he hit upon was pocket calculators. On a plane ride with Jack Kilby, Haggerty sketched out his idea and handed Kilby his marching orders: Build a handheld calculator that can do the same tasks as the thousand-dollar clunkers that sit on office desks. Make it efficient enough to run on batteries, small enough to put into a shirt pocket, and cheap enough to buy on impulse. In 1967 Kilby and his team produced almost what Haggerty envisioned. It could do only four tasks (add, subtract, multiply, and divide) and was a bit heavy (more than two pounds) and not very cheap ($150).21 But it was a huge success. A new market had been created for a device people had not known they needed. And following the inevitable trajectory, it kept getting smaller, more powerful, and cheaper. (Location 3242)
- That became the pattern for electronic devices. Every year things got smaller, cheaper, faster, more powerful. This was especially true—and important—because two industries were growing up simultaneously, and they were intertwined: the computer and the microchip. “The synergy between a new component and a new application generated an explosive growth for both,” Noyce later wrote.22 The same synergy had happened a half century earlier when the oil industry grew in tandem with the auto industry. There was a key lesson for innovation: Understand which industries are symbiotic so that you can capitalize on how they will spur each other on. (Location 3252)
- “The complexity for minimum component costs has increased at a rate of roughly a factor of two per year,” he noted. “There is no reason to believe it will not remain nearly constant for at least ten years.”23 Roughly translated, he was saying that the number of transistors that could be crammed, cost-effectively, onto a microchip had been doubling every year, and he expected it to do so for at least the next ten years. (Location 3263)
- Noyce decided that Fairchild would sell its simplest microchips for less than they cost to make. Moore called the strategy “Bob’s unheralded contribution to the semiconductor industry.” Noyce knew that the low price would cause device makers to incorporate microchips into their new products. He also knew that the low price would stimulate demand, high-volume production, and economies of scale, which would turn Moore’s Law into a reality. (Location 3275)
- The first name that Noyce and Moore chose for their new company was NM Electronics, their initials. That was not very exciting. After many clunky suggestions—Electronic Solid State Computer Technology Corp. was one—they finally decided on Integrated Electronics Corp. That wasn’t very thrilling, either, but it had the virtue that it could be abridged—as Intel. (Location 3348)
- It also helps to remember that, from his early days as a student, Noyce loved madrigal singing. Every Wednesday evening he attended rehearsals of his twelve-voice group. Madrigals don’t rely on lead singers and soloists; the polyphonic songs weave multiple voices and melodies together, none of them dominant. “Your part depends on [the others’ and] it always supports the others,” Noyce once explained. (Location 3366)
- Saturday Night Live skit, to be groovy. He grew his sideburns (Location 3470)
- Grove’s mantra was “Success breeds complacency. Complacency breeds failure. Only the paranoid survive.” Noyce and Moore may not have been paranoid, but they were never complacent. (Location 3480)
- Hoff envisioned, as did Noyce and others, an alternative approach: creating a general-purpose chip that could be instructed, or programmed, to do a variety of different applications as desired. In other words, a general-purpose computer on a chip. (Location 3486)
- Because it was essentially a computer processor on a chip, the new device was dubbed a microprocessor. In November 1971 Intel unveiled the product, the Intel 4004, to the public. It took out ads in trade magazines announcing “a new era of integrated electronics—a micro-programmable computer on a chip!” It was priced at $200, (Location 3516)
- 1971 the region got a new moniker. Don Hoefler, a columnist for the weekly trade paper Electronic News, began writing a series of columns entitled “Silicon Valley USA,” and the name stuck. (Location 3535)
- The evolution of microchips led to devices that were, as Moore’s Law forecast, smaller and more powerful each year. But there was another impetus that would drive the computer revolution and, eventually, the demand for personal computers: the belief that computers weren’t merely for number-crunching. They could and should be fun for people to use. (Location 3550)
- The intricate pranks devised by MIT students—putting a live cow on the roof of a dorm, a plastic cow on the Great Dome of the main building, or causing a huge balloon to emerge midfield during the Harvard-Yale game—were known as hacks. “We at TMRC use the term ‘hacker’ only in its original meaning, someone who applies ingenuity to create a clever result, called a ‘hack,’ ” the club proclaimed. “The essence of a ‘hack’ is that it is done quickly, and is usually inelegant.” (Location 3570)
- About the size of three refrigerators, the PDP-1 was the first computer to be designed for direct interaction with the user. (Location 3583)
- Spacewar highlighted three aspects of the hacker culture that became themes of the digital age. First, it was created collaboratively. “We were able to build it together, working as a team, which is how we liked to do things,” Russell said. Second, it was free and open-source software. “People asked for copies of the source code, and of course we gave them out.” Of course—that was in a time and place when software yearned to be free. Third, it was based on the belief that computers should be personal and interactive. “It allowed us to get our hands on a computer and make it respond to us in real time,” said Russell.10 (Location 3651)
- Innovation can be sparked by engineering talent, but it must be combined with business skills to set the world afire. (Location 3700)
- He decided to name the new company Syzygy, a barely pronounceable term for when three celestial bodies are in a line. Fortunately, that name was not available because a hippie candle-making commune had registered it. So Bushnell decided to call his new venture Atari, adopting a term from the Japanese board game Go. (Location 3711)
- At a trade show, Bushnell had checked out the Magnavox Odyssey, a primitive console for playing games on home television sets. One of the offerings was a version of Ping-Pong. “I thought it was kind of crappy,” Bushnell said years later, after he had been sued for stealing its idea. “It had no sound, no score, and the balls were square. But I noticed some people were having some fun with it.” When he arrived back at Atari’s little rented office in Santa Clara, he described the game to Alcorn, sketched out some circuits, and asked him to build an arcade version of it. He told Alcorn he had signed a contract with GE to make the game, which was untrue. Like many entrepreneurs, Bushnell had no shame about distorting reality in order to motivate people. “I thought it would be a great training program for Al.”20 Alcorn got a prototype wired up in a few weeks, completing it at the beginning of September 1972. With his childlike sense of fun, he came up with enhancements that turned the monotonous blip bouncing between paddles into something amusing. The lines he created had eight regions so that when the ball hit smack in the center of a paddle it bounced back straight, but as it hit closer to the paddle’s edges it would fly off at angles. That made the game more challenging and tactical. He also created a scoreboard. And in a stroke of simple genius, he added just the right “thonk” sound from the sync generator to sweeten the experience. Using a $75 Hitachi black-and-white TV set, Alcorn hard-wired the components together inside a four-foot-tall wooden cabinet. Like Computer Space, the game did not use a microprocessor or run a line of computer code; it was all done in hardware with the type of digital logic design used by television engineers. Then he slapped on a coin box taken from an old pinball machine, and a star was born.21 Bushnell dubbed it Pong. (Location 3724)
- Alcorn got a prototype wired up in a few weeks, completing it at the beginning of September 1972. With his childlike sense of fun, he came up with (Location 3731)
- Pong, by contrast, was simple enough that a beer-sloshed barfly or stoned sophomore could figure it out after midnight. There was only one instruction: “Avoid missing ball for high score.” Consciously or not, Atari had hit upon one of the most important engineering challenges of the computer age: creating user interfaces that were radically simple and intuitive. (Location 3741)
- it took to the next level the casual style of Silicon Valley startups. (Location 3774)
- At its core were certain principles: authority should be questioned, hierarchies should be circumvented, nonconformity should be admired, and creativity should be nurtured. (Location 3783)
- Innovation requires having at least three things: a great idea, the engineering talent to execute it, and the business savvy (plus deal-making moxie) to turn it into a successful product. (Location 3795)
- For the Internet, this was especially interesting, for it was built by a partnership among three groups: the military, universities, and private corporations. (Location 3817)
- The advent of World War II would change that, producing an explosion of new technologies, with Vannevar Bush leading the way. Worried that America’s military was lagging in technology, he mobilized Harvard president James Bryant Conant and other scientific leaders to convince President Franklin Roosevelt to form the National Defense Research Committee and then the military’s Office of Scientific Research and Development, both of which he headed. (Location 3844)
- The creation of a triangular relationship among government, industry, and academia was, in its own way, one of the significant innovations that helped produce the technological revolution of the late twentieth century. The Defense Department and National Science Foundation soon became the prime funders of much of America’s basic research, spending as much as private industry during the 1950s through the 1980s.I The return on that investment was huge, leading not only to the Internet but to many of the pillars of America’s postwar innovation and economic boom. (Location 3877)
- In searching for fathers of the Internet, the best person to start with is a laconic yet oddly charming psychologist and technologist, with an open-faced grin and show-me attitude, named Joseph Carl Robnett Licklider, born in 1915 and known to everyone as “Lick.” He pioneered the two most important concepts underlying the Internet: decentralized networks that would enable the distribution of information to and from anywhere, and interfaces that would facilitate human-machine interaction in real time. Plus, he was the founding director of the military office that funded the ARPANET, and he returned for a second stint a decade later when protocols were created to weave it into what became the Internet. Said one of his partners and protégés, Bob Taylor, “He was really the father of it all.”12 (Location 3890)
- Unlike some of his MIT colleagues, Wiener believed that the most promising path for computer science was to devise machines that would work well with human minds rather than try to replace them. “Many people suppose that computing machines are replacements for intelligence and have cut down the need for original thought,” Wiener wrote. “This is not the case.”14 The more powerful the computer, the greater the premium that will be placed on connecting it with imaginative, creative, high-level human thinking. Licklider became an adherent of this approach, which he later called “man-computer symbiosis.” (Location 3908)
- Up until then, when you wanted a computer to perform a task, you had to submit a stack of punch cards or a tape to the computer’s operators, as if handing an offering to the priests who shielded an oracle. This was known as “batch processing,” and it was annoying. It could take hours or even days to get results back; any little mistake might mean having to resubmit your cards for another run; and you might not be able to touch or even see the computer itself. (Location 3944)
- This provided users with an enchanting experience: you could have a hands-on and real-time interaction with a computer, like a conversation. “We had a kind of little religion growing here about how this was going to be totally different from batch processing,” said Licklider.19 It was a key step toward a direct human-computer partnership or symbiosis. (Location 3950)
- One mission at Lincoln Laboratory was developing computers for an air defense system that would provide early warning of an enemy attack and coordinate a response. It was known as SAGE, for Semi-Automatic Ground Environment, and it cost more money and employed more people than the Manhattan Project that built the atom bomb. For it to work, the SAGE system needed to enable its users to have instant interactions with its computers. When an enemy missile or bomber was on the way, there would be no time for batch processing of the calculations. (Location 3959)
- These ideas formed the basis for one of the most influential papers in the history of postwar technology, titled “Man-Computer Symbiosis,” which Licklider published in 1960. “The hope is that, in not too many years, human brains and computing machines will be coupled together very tightly,” he wrote, “and that the resulting partnership will think as no human brain has ever thought and process data in a way not approached by the information-handling machines we know today.” (Location 3976)
- One of Licklider’s assignments at BBN was to lead a team tasked with figuring out how computers could transform libraries. He dictated his final report, “Libraries of the Future,” (Location 3991)
- Licklider also predicted something that was counterintuitive but has turned out to be pleasantly true: that digital information would not completely replace print. “As a medium for the display of information, the printed page is superb,” he wrote. “It affords enough resolution to meet the eye’s demand. It presents enough information to occupy the reader for a convenient quantum of time. It offers great flexibility of font and format. It lets the reader control the mode and rate of inspection. It is small, light, movable, cuttable, clippable, pastable, replicable, disposable, and inexpensive.” (Location 4002)
- An even more efficient method is packet switching, a special type of store-and-forward switching in which the messages are broken into bite-size units of the exact same size, called packets, which are given address headers describing where they should go. These packets are then sent hopping through the network to their destination by being passed along from node to node, using whatever links are most available at that instant. If certain links start getting clogged with too much data, some of the packets will be routed to alternative paths. When all the packets get to their destination node, they are reassembled based on the instructions in the headers. “It’s like breaking a long letter into dozens of postcards, each numbered and addressed to the same place,” explained Vint Cerf, one of the Internet’s pioneers. “Each may take different routes to get to the destination, and then they’re reassembled.” (Location 4187)
- AT&T was stymied by the innovator’s dilemma. It balked (Location 4239)
- Paul Baran, who did deserve to be known as the father of packet switching, came forward to say that “the Internet is really the work of a thousand people,” and he pointedly declared that most people involved did not assert claims of credit. “It’s just this one little case that seems to be an aberration,” he added, referring disparagingly to Kleinrock. (Location 4316)
- Innovation is driven by people who have both good theories and the opportunity to be part of a group that can implement them. (Location 4332)
- The Kleinrock controversy is interesting because it shows that most of the Internet’s creators preferred—to use the metaphor of the Internet itself—a system of fully distributed credit. They instinctively isolated and routed around any node that tried to claim more significance than the others. The Internet was born of an ethos of creative collaboration and distributed decision making, and its founders liked to protect that heritage. It became ingrained in their personalities—and in the DNA of the Internet itself. (Location 4333)
- One of the commonly accepted narratives of the Internet is that it was built to survive a nuclear attack. This enrages many of its architects, including Bob Taylor and Larry Roberts, who insistently and repeatedly debunked this origin myth. (Location 4337)
- it was useful—and fun—to get together in person, interfacing in the literal sense of that word. (Location 4451)
- Unlike Kleinrock, Crocker rarely used the pronoun I; he was more interested in distributing credit than claiming it. His sensitivity toward others gave him an intuitive feel for how to coordinate a group without trying to centralize control or authority, which was well suited to the network model they were trying to invent. (Location 4455)
- Crocker realized that he needed an unassertive name for the list of suggestions and practices. “To emphasize the informal nature, I hit upon this silly little idea of calling every one of them a ‘Request for Comments’—no matter whether it really was a request.” It was the perfect phrase to encourage Internet-era collaboration—friendly, not bossy, inclusive, and collegial. (Location 4473)
- It was thus that in the second half of 1969—amid the static of Woodstock, Chappaquiddick, Vietnam War protests, Charles Manson, the Chicago Eight trial, and Altamont—the culmination was reached for three historic enterprises, each in the making for almost a decade. NASA was able to send a man to the moon. Engineers in Silicon Valley were able to devise a way to put a programmable computer on a chip called a microprocessor. And ARPA created a network that could connect distant computers. Only the first of these (perhaps the least historically significant of them?) made headlines. (Location 4505)
- So who does deserve the most credit for inventing the Internet? (Hold the inevitable Al Gore jokes. We will get to his role—yes, he did have one—in chapter 10.) As with the question of who invented the computer, the answer is that it was a case of collaborative creativity. (Location 4577)
- The process of technological development is like building a cathedral. Over the course of several hundred years new people come along and each lays down a block on top of the old foundations, each saying, “I built a cathedral.” Next month another block is placed atop the previous one. Then comes along an historian who asks, “Well, who built the cathedral?” Peter added some stones here, and Paul added a few more. If you are not careful, you can con yourself into believing that you did the most important part. But the reality is that each contribution has to follow onto previous work. Everything is tied to everything else. (Location 4580)
- Innovation is not a loner’s endeavor, and the Internet was a prime example. “With computer networks, the loneliness of research is supplanted by the richness of shared research,” (Location 4591)
- The idea of a personal computer, one that ordinary individuals could get their hands on and take home, was envisioned in 1945 by Vannevar Bush. (Location 4619)
- “Consider a future device for individual use, which is a sort of mechanized private file and library. . . . A memex is a device in which an individual stores all his books, records, and communications, and which is mechanized so that it may be consulted with exceeding speed and flexibility. It is an enlarged intimate supplement to his memory.” (Location 4623)
- Hippie communalism and libertarian politics formed the roots of the modern cyberrevolution. . . . (Location 4706)
- “Computers did more than politics did to change society.” (Location 4723)
- But the pursuit of artificial intelligence didn’t excite Engelbart, who never lost sight of his mission to augment human intelligence by creating machines like Bush’s memex that could work closely with people and help them organize information. This goal, he later said, was born out of his respect for the “ingenious invention” that was the human mind. Instead of trying to replicate that on a machine, Engelbart focused on how “the computer could interact with the different capabilities that we’ve already got.” (Location 4829)
- He began by explaining that he was not seeking to replace human thought with artificial intelligence. Instead he argued that the intuitive talents of the human mind should be combined with the processing abilities of machines to produce “an integrated domain where hunches, cut-and-try, intangibles, and the human ‘feel for a situation’ usefully co-exist with powerful concepts, streamlined terminology and notation, sophisticated methods, and high-powered electronic aids.” (Location 4835)
- The result, at once both simple and profound, was a classic physical expression of the augmentation ideal and the hands-on imperative. It made use of the human talent of mind-hand-eye coordination (something robots are not good at) to provide a natural interface with a computer. (Location 4870)
- the innovator’s dilemma: he saw the future filled with shadowy creatures that threatened to gnaw away at Xerox’s copier business. (Location 5048)
- “The best way to predict the future is to invent it.” (Location 5051)
- “A Personal Computer for Children of All Ages,” (Location 5065)
- “The Analytical Engine weaves algebraical patterns just as the Jacquard loom weaves flowers and leaves.” (Location 5066)
- Kay showed he was in the camp of those who saw personal computers primarily as tools for individual creativity rather than as networked terminals for collaboration. (Location 5068)
- In May 1972 he made his pitch to Xerox PARC’s hardware bosses to build thirty so that they could be tested in classrooms to see if students could do simple programming tasks on them. (Location 5083)
- That is why Engelbart, even though he was a prescient theorist, was not truly a successful innovator: he kept adding functions and instructions and buttons and complexities to his system. Kay made things easier, and in so doing showed why the ideal of simplicity—making products that humans find convivial and easy to use—was central to the innovations that made computers personal. (Location 5131)
- “Computers are mostly used against people instead of for people; used to control people instead of to free them; Time to change all that—we need a PEOPLE’S COMPUTER COMPANY.” (Location 5206)
- he later worried that subsequent generations were growing up with sealed devices that couldn’t be explored.III “I learned electronics as a kid by messing around with old radios that were easy to tamper with because they were designed to be fixed.” (Location 5233)
- Tools for Conviviality, by Ivan Illich, (Location 5301)
- “The roots of the personal computer can be found in the Free Speech Movement that arose at Berkeley in 1964 and in the Whole Earth Catalog, which did the marketing for the do-it-yourself ideals behind the personal computer movement.” (Location 5313)
- Public awareness is an important component of innovation. A computer created in, say, a basement in Iowa that no one writes about becomes, for history, like a tree falling in Bishop Berkeley’s uninhabited forest; it’s not obvious that it makes a sound. The Mother of All Demos helped Engelbart’s innovations catch on. That is why product launches are so important. The MITS machine might have languished with the unsold calculators in Albuquerque, if Roberts had not previously befriended Les Solomon of Popular Electronics, which was to the Heathkit set what Rolling Stone was for rock fans. (Location 5386)
- According to Solomon, his daughter, a Star Trek junkie, suggested it be named after the star that the spaceship Enterprise was visiting that night, Altair. And so the first real, working personal computer for home consumers was named the Altair 8800. (Location 5396)
- “To my mind,” Bill Gates would later declare, “the Altair is the first thing that deserves to be called a personal computer.” (Location 5401)
- People were sending checks to a company they had never heard of, in a town whose name they couldn’t spell, in hopes of eventually getting a box of parts that they could solder together that would, if all went well, make some lights blink on and off based on information they had painstakingly entered using toggle switches. With the passion of hobbyists, they wanted a computer of their own—not a shared device or one that would network with other people but one that they could play with by themselves in their bedroom or basement. (Location 5405)
- In a power-to-the-people move, computers were wrested from the sole control of corporations and the military and placed into the hands of individuals, making them tools for personal enrichment, productivity, and creativity. (Location 5410)
- “The dystopian society envisioned by George Orwell in the aftermath of World War II, at about the same time the transistor was invented, has completely failed to materialize,” the historians Michael Riordan and Lillian Hoddeson wrote, “in large part because transistorized electronic devices have empowered creative individuals and nimble entrepreneurs far more than Big Brother.” (Location 5412)
- By the time of the third Homebrew meeting, in April 1975, he had made an amusing discovery. He had written a program to sort numbers, and while he was running it, he was listening to a weather broadcast on a low-frequency transistor radio. The radio started going zip-zzziiip-ZZZIIIPP at different pitches, and Dompier said to himself, “Well what do you know! My first peripheral device!” So he experimented. “I tried some other programs to see what they sounded like, and after about eight hours of messing around I had a program that could produce musical tones and actually make music.” (Location 5422)
- Dompier then had his Altair produce a version of “Daisy Bell (Bicycle Built for Two),” which had been the first song ever played by a computer, at Bell Labs on an IBM 704 in 1961, and was reprised in 1968 by HAL when it was being dismantled in Stanley Kubrick’s 2001: A Space Odyssey. “Genetically inherited,” was how Dompier described the song. The members of the Homebrew Club had found a computer they could take home and make do all sorts of beautiful things, including, as Ada Lovelace had predicted, rendering music. (Location 5430)
